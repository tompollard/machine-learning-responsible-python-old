---
title: "Interpretability"
teaching: 0
exercises: 0
questions:
- "Do we need interpretable models?"
objectives:
- "Consider the importance of interpretability in machine learning."
keypoints:
- "It is not clear that interpretability is necessary."
---

## Interpretability of machine learning models

Machine learning models - in particular neural networks - are often criticised for being "black boxes". If a model is making a prediction, many of us would like to know how the decision was reached. With this understanding, we gain trust. 

The requirement for interpretability is even making its way into legal governance. The European Union General Data Protection (GDPR)) for example, states that "[the data subject should have] the right ... to obtain an explanation of the decision reached".

While at face value interpretability seems like something that we would want in our models, the demand for interpretability is not without controversy. https://arxiv.org/pdf/1702.08608.pdf

If our doctor or nurse recommends paracetamol (acetaminophen) for pain management, most of us would accept the suggestion without question. This is despite the action of paracetamol at a molecular level being unclear [https://pubmed.ncbi.nlm.nih.gov/15662292/]. 

Some would also argue that an explainable model creates trust where it is not well founded. A model may convince us that its reasoning is sound, even where it is not.

Examples, and discussion.

{% include links.md %}
